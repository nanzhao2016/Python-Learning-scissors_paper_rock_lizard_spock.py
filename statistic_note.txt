Online Statistics Education: An Interactive Multimedia Course of Study
http://onlinestatbook.com/2/index.html


Part1. Introduction

Inferential Statistics
In statistics, we often rely on a sample --- that is, a small subset of a larger set of data --- to draw inferences about the larger set. The larger set is known as the population.

Simple Random Sampling
Such sampling requires every member of the population to have an equal chance of being selected into the sample. 
In addition, the selection of one member must be independent of the selection of every other member. 
That is, picking one member from the population must not increase or decrease the probability of picking any other member (relative to the others). 

Random sampling chooses a sample by pure chance

More Complex Sampling
1. Random Assignment : used for experiment studies: experimental group vs control group, 
Random assignment is critical for the validity of an experiment. 
In experimental research of this kind, failure to assign subjects randomly to groups is generally more serious than having a non-random sample.
Failure to randomize (the former error) invalidates the experimental findings. A non-random sample (the latter error) simply restricts the generalizability of the results.

2. Stratified Sampling: 
In stratified sampling, you first identify members of your sample who belong to each group. 
Then you randomly sample from each of those subgroups in such a way that the sizes of the subgroups in the sample are proportional to their sizes in the population. 

Stratified sampling is more likely to be representative of the population than random sampling.
The only way to eliminate uncertainty is to obtain data from the whole population. You can reduce uncertainty with a large sample.
Using a random sample:
is to accept some uncertainty about the conclusions. 

enables you to calculate statistics. 

is to risk drawing the wrong conclusions about the population. 
 
A biased sample is one that 

will likely have groups from the population over-represented or under-represented due to systematic sampling factors. 
Bias is defined by the procedure for drawing the sample, not by the result.

Levels of Measurement
nominal, ordinal, interval, and ratio scales
ordinal scales fail to capture important informations
In particular, the difference between two levels of an ordinal scale cannot be assumed to be the same as the difference between two other levels.
Statisticians express this point by saying that the differences between adjacent scale values do not necessarily represent equal intervals on the underlying scale giving rise to the measurements.

Interval scales are numerical scales in which intervals have the same interpretation throughout. 
Interval scales are not perfect, however. In particular, they do not have a true zero point even if one of the scaled values happens to carry the name "zero." 
For example, there is no sense in which the ratio of 40 to 20 degrees Fahrenheit is the same as the ratio of 100 to 50 degrees; no interesting physical property is preserved across the two ratios. 
The ratio scale of measurement is the most informative scale. It is an interval scale with the additional property that its zero position indicates the absence of the quantity being measured. 
You can think of a ratio scale as the three earlier scales rolled up in one. Like a nominal scale, it provides a name or category for each object (the numbers serve as labels). 
Like an ordinal scale, the objects are ordered (in terms of the ordering of the numbers). Like an interval scale, the same difference at two places on the scale has the same meaning. 
And in addition, the same ratio at two places on the scale also carries the same meaning.

Rating scales are used frequently in psychological research.
Typically these ratings are made on a 5-point or a 7-point scale. These scales are ordinal scales since there is no assurance that a given difference represents the same thing across the range of the scale. 

Does it make sense to compute the mean of numbers measured on an ordinal scale? This is a difficult question, one that statisticians have debated for decades. 
The prevailing (but by no means unanimous) opinion of statisticians is that for almost all practical situations, the mean of an ordinally-measured variable is a meaningful statistic. 
However, as you will see in the simulation, there are extreme situations in which computing the mean of an ordinally-measured variable can be very misleading.

Ordinal scales preserve the order of the values, but not the differences between values.

http://onlinestatbook.com/2/introduction/measurement_demo.html
A difference between the means of two groups on an ordinal rating scale : 
For all but the most extraordinary situations, differences between means on interval scales are meaningful. 
, in extreme circumstances, it is possible for the difference on an ordinal scale to be in the oppostite direction from the difference on an interval scale.


Distributions
frequency distribution VS probability distribution
Distributions of Discrete Variables

Table 1. Frequencies in the Bag of M&M's
Color	Frequency
Brown	17
Red		18
Yellow	7
Green	7
Blue		2
Orange	4

This table is called a frequency table and it describes the distribution of M&M color frequencies. 
We call Figure 2 a probability distribution because if you choose an M&M at random, the probability of getting, say, a brown M&M is equal to the proportion of M&M's that are brown (0.30).

Chance factors involving the machines used by the manufacturer introduce random variation into the different bags produced. Some bags will have a distribution of colors that is close to Figure 2; 
others will be further away.

Continuous Variables
histogram can be used to show the grouped frequency distribution

Probability Densities (continuous distribution)
normal distribution: The Y-axis in the normal distribution represents the "density of probability." Intuitively, it shows the chance of obtaining values near corresponding points on the X-axis.

the curve that describes a continuous distribution (like the normal distribution). 
First, the area under the curve equals 1. 
Second, the probability of any exact value of X is 0. 
Finally, the area under the curve and bounded between two given points on the X-axis is the probability that a number chosen at random will fall between the two points.

A distribution with two peaks is called a bimodal distribution.
Distributions also differ from each other in terms of how large or "fat" their tails are. 
The upper distribution has relatively more scores in its tails; its shape is called leptokurtic. The lower distribution has relatively fewer scores in its tails; its shape is called platykurtic.


Part2. Graphing Distributions
Graphing Qualitative Variables

Pie charts are effective for displaying the relative frequencies of a small number of categories. 
Pie charts can also be confusing when they are used to compare the outcomes of two different surveys or experiments. 
In an influential book on the use of graphs, Edward Tufte asserted, "The only worse design than a pie chart is several of them."
we note that it is a serious mistake to use a line graph when the X-axis contains merely qualitative variables.

Pie charts and bar charts can both be effective methods of portraying qualitative data. 
Bar charts are better when there are more than just a few categories and for comparing two or more distributions

Quantitative Variables

Stem and Leaf Displays
Whether your data can be suitably represented by a stem and leaf graph depends on whether they can be rounded without loss of important information. 

Histograms

A histogram is a graphical method for displaying the shape of a distribution. It is particularly useful when there are a large number of observations. 

Table 1. Grouped Frequency Distribution of Psychology Test Scores
Interval's Lower Limit	Interval's Upper Limit	Class Frequency
39.5	49.5	3
49.5	59.5	10
59.5	69.5	53
69.5	79.5	107
79.5	89.5	147
89.5	99.5	130
99.5	109.5	78
109.5	119.5	59
119.5	129.5	36
129.5	139.5	11s
139.5	149.5	6
149.5	159.5	1
159.5	169.5	1

the range of scores was broken into intervals, called class intervals,
Class intervals of width 10 provide enough detail about the distribution to be revealing without making the graph too "choppy." 
More information on choosing the widths of class intervals is presented later in this section. 
Placing the limits of the class intervals midway between two numbers (e.g., 49.5) ensures that every score will fall in an interval rather than on the boundary between intervals.
The histogram makes it plain that most of the scores are in the middle of the distribution, with fewer scores in the extremes. 
Using whole numbers as boundaries avoids a cluttered appearance, and is the practice of many computer programs that create histograms. 
Note also that some computer programs label the middle of each interval rather than the end points.
Histograms can be based on relative frequencies instead of actual frequencies. Histograms based on relative frequencies show the proportion of scores in each interval rather than the number of scores. 

widths of the class intervals = bin widths

Your choice of bin width determines the number of class intervals. This decision, along with the choice of starting point for the first interval, affects the shape of the histogram.
Sturges' rule is to set the number of intervals as close as possible to 1 + Log2(N), where Log2(N) is the base 2 log of the number of observations. 
The formula can also be written as 1 + 3.3 Log10(N)
We prefer the Rice rule, which is to set the number of intervals to twice the cube root of the number of observations ******

The best advice is to experiment with different choices of width, and to choose a histogram according to how well it communicates the shape of the distribution.

Frequency Polygons
Frequency polygons are useful for comparing distributions. This is achieved by overlaying the frequency polygons drawn for different data sets. 
Frequency polygons are better at comparing distributions because two frequency polygons can be displayed in the same graph without obscuring each other. 
Both histograms and frequency polygons show the shape of the distribution. Neither necessarily reveals the exact values in a distribution.
Frequency polygons are a graphical device for understanding the shapes of distributions. They serve the same purpose as histograms, but are especially helpful for comparing sets of data.

Boxplot
Since half the scores in a distribution are between the hinges (recall that the hinges are the 25th and 75th percentiles), 
we see that half the women's times are between 17 and 20 seconds, whereas half the men's times are between 19 and 25.5. 
We also see that women generally named the colors faster than the men did, although one woman was slower than almost all of the men.

a distribution with a positive skew would have a longer whisker in the positive direction than in the negative direction. 
A larger mean than median would also indicate a positive skew. Box plots are good at portraying extreme values and are especially good at showing differences between distributions. 
However, many of the details of a distribution are not revealed in a box plot, and to examine these details one should create a histogram and/or a stem and leaf display.


Part3. Summarizing Distributions
Median and Mean

51) the point on which a distribution would balance, (2) the value whose average absolute deviation from all the other values is minimized, and (3) the value whose average squared difference from all the other values is minimized. 
From the simulation in this chapter, you discovered (we hope) that the mean is the point on which a distribution would balance, the median is the value that minimizes the sum of absolute deviations, and the mean is the value that minimizes the sum of the squared deviations.


Additional Measures of Central Tendency
Trimean
The trimean is a weighted average of the 25th percentile, the 50th percentile, and the 75th percentile. Letting P25 be the 25th percentile, P50 be the 50th and P75 be the 75th percentile, the formula for the trimean is:
Trimean = (P25 + 2P50 + P75)/4

Geometric Mean
The geometric mean is computed by multiplying all the numbers together and then taking the nth root of the product.
The geometric mean is an appropriate measure to use for averaging rates. For example, consider a stock portfolio that began with a value of $1,000 and had annual returns of 13%, 22%, 12%, -5%, and -13%. Table 4 shows the value after each of the five years.
Table 4. Portfolio Returns
Year	Return	Value
1	13%	1,130
2	22%	1,379
3	12%	1,544
4	-5%	1,467
5	-13%	1,276
The question is how to compute average annual rate of return. The answer is to compute the geometric mean of the returns. Instead of using the percents, each return is represented as a multiplier indicating how much higher the value is after the year. 
This multiplier is 1.13 for a 13% return and 0.95 for a 5% loss. The multipliers for this example are 1.13, 1.22, 1.12, 0.95, and 0.87. 
The geometric mean of these multipliers is 1.05. Therefore, the average annual rate of return is 5%. Table 5 shows how a portfolio gaining 5% a year would end up with the same value ($1,276) as shown in Table 4.

Trimmed Mean
To compute a trimmed mean, you remove some of the higher and lower scores and compute the mean of the remaining scores. A mean trimmed 10% is a mean computed with 10% of the scores trimmed off: 5% from the bottom and 5% from the top. 
A mean trimmed 50% is computed by trimming the upper 25% of the scores and the lower 25% of the scores and computing the mean of the remaining scores. The trimmed mean is similar to the median which, in essence, trims the upper 49+% and the lower 49+% of the scores. 
Therefore the trimmed mean is a hybrid of the mean and the median. To compute the mean trimmed 20% for the touchdown pass data shown in Table 1, you remove the lower 10% of the scores (6, 9, and 12) as well as the upper 10% of the scores (33, 33, and 37) and compute the mean of the remaining 25 scores. This mean is 20.16.


Fortunately, there is no need to summarize a distribution with a single number. When the various measures differ, our opinion is that you should report the mean, median, and either the trimean or the mean trimmed 50%. 
Sometimes it is worth reporting the mode as well. In the media, the median is usually reported to summarize the center of skewed distributions. 

Measures of Variability

What is Variability?
Variability refers to how "spread out" a group of scores is.
There are four frequently used measures of variability: the range, interquartile range, variance, and standard deviation.
 
Range
The range is the simplest measure of variability to calculate, and one you have probably encountered many times in your life. 
The range is simply the highest score minus the lowest score.
Let’s take a few examples. What is the range of the following group of numbers: 10, 2, 5, 6, 7, 3, 4? 
Well, the highest number is 10, and the lowest number is 2, so 10 - 2 = 8. The range is 8.

Interquartile Range
The interquartile range (IQR) is the range of the middle 50% of the scores in a distribution. It is computed as follows:
IQR = 75th percentile - 25th percentile
Recall that in the discussion of box plots, the 75th percentile was called the upper hinge and the 25th percentile was called the lower hinge.
Using this terminology, the interquartile range is referred to as the H-spread.
 
A related measure of variability is called the semi-interquartile range. The semi-interquartile range is defined simply as the interquartile range divided by 2. 
If a distribution is symmetric, the median plus or minus the semi-interquartile range contains half the scores in the distribution. 

Variance
Variability can also be defined in terms of how close the scores in the distribution are to the middle of the distribution. 
Using the mean as the measure of the middle of the distribution, the variance is defined as the average squared difference of the scores from the mean.
One thing that is important to notice is that the mean deviation from the mean is 0. This will always be the case. 

where σ2 is the variance, μ is the mean, and N is the number of numbers. 
If the variance in a sample is used to estimate the variance in a population, then the previous formula underestimates the variance and the following formula should be used:

N-1 to calculate the variance
where s2 is the estimate of the variance and M is the sample mean. 
Note that M is the mean of a sample taken from a population with a mean of μ. Since, in practice, the variance is usually computed in a sample, this formula is most often used. 

Standard Deviation
The standard deviation is simply the square root of the variance.
he standard deviation is an especially useful measure of variability when the distribution is normal or approximately normal (see Chapter on Normal Distributions) because the proportion of the distribution within a given number of standard deviations from the mean can be calculated. 
For example, 68% of the distribution is within one standard deviation of the mean and approximately 95% of the distribution is within two standard deviations of the mean.
Therefore, if you had a normal distribution with a mean of 50 and a standard deviation of 10, then 68% of the distribution would be between 50 - 10 = 40 and 50 +10 =60. Similarly, about 95% of the distribution would be between 50 - 2 x 10 = 30 and 50 + 2 x 10 = 70. 
The symbol for the population standard deviation is σ; the symbol for an estimate computed in a sample is s. 

Assume you repeatedly sampled 4 numbers from this population with a variance of 2 and, for each sample, estimated the variance using the average squared difference from the sample mean. 
What would the mean of these variance estimates be?
It would approach 1.5 as the number of samples increases to a very large number. 
This is equal to (N-1)/N = .75 times the population variance of 2.

Shapes of Distributions
The relationship between skew and the relative size of the mean and median led the statistician Pearson to propose the following simple and convenient numerical index of skew:

3(mean-median)/σ

Just as there are several measures of central tendency, there is more than one measure of skew. 
Although Pearson's measure is a good one, the following measure is more commonly used. It is sometimes referred to as the third moment about the mean.

sigma[(X-µ)3/σ3]

Kurtosis
The following measure of kurtosis is similar to the definition of skew. 
The value "3" is subtracted to define "no kurtosis" as the kurtosis of a normal distribution. Otherwise, a normal distribution would have a kurtosis of 3.

sigma[(X-µ)4/σ4]-3

Effects of Linear Transformations

To sum up, if a variable X has a mean of μ, a standard deviation of σ, and a variance of σ2, then a new variable Y created using the linear transformation
Y = bX + A

will have a mean of bμ+A, a standard deviation of bσ, and a variance of b2σ2.

In R, calculate trimmed mean: mean(data, trim=percentage)

Notice that the expression for the difference is the same as the formula for the sum.

(σ2)diff = (σ2)M+(σ2)F


More generally, the variance sum law can be written as follows:

(σ2)X+/-Y = (σ2)X+(σ2)Y

which is read: The variance of X plus or minus Y is equal to the variance of X plus the variance of Y.
These formulas for the sum and difference of variables given above only apply when the variables are independent. 


Part4. Describing Bivariate Data

Introduction to Bivariate Data

Bivariate data is data for which there are two variables for each observation. As an example, the following bivariate data show the ages of husbands and wives of 10 married couples.
 
Husband	36	72	37	36	51	50	47	50	37	41
Wife			35	67	33	35	50	46	47	42	36	41

A scatter plot of two variables shows the values of one variable on the Y axis and the values of the other variable on the X axis. 
Scatter plots are well suited for revealing the relationship between two variables.
parabola
A statistical measure of the strength of the relationship between two quantitative variables that takes these factors into account is the subject of the section "Values of Pearson's Correlation."


Values of the Pearson Correlation
The Pearson product-moment correlation coefficient is a measure of the strength of the linear relationship between two variables. 
It is referred to as Pearson's correlation or simply as the correlation coefficient.

The symbol for Pearson's correlation is "ρ" when it is measured in the population and "r" when it is measured in a sample. 
Pearson's r can range from -1 to 1. 
An r of -1 indicates a perfect negative linear relationship between variables, an r of 0 indicates no linear relationship between variables, and an r of 1 indicates a perfect positive linear relationship between variables.

Properties of Pearson's r
Pearson's correlation is symmetric in the sense that the correlation of X with Y is the same as the correlation of Y with X. 
A critical property of Pearson's r is that it is unaffected by linear transformations.
 
This means that multiplying a variable by a constant and/or adding a constant does not change the correlation of that variable with other variables. 

Computing Pearson's r
We begin by computing the mean for X and subtracting this mean from all values of X. The new variable is called "x". 
The variable "y" is computed similarly. The variables x and y are said to be deviation scores because each score is a deviation from the mean.

Before proceeding with the calculations, let's consider why the sum of the xy column reveals the relationship between X and Y. 
If there were no relationship between X and Y, then positive values of x would be just as likely to be paired with negative values of y as with positive values. 
This would make negative values of xy as likely as positive values and the sum would be small.

Pearson's correlation is computed by dividing the sum of the xy column (Σxy) by the square root of the product of the sum of the x2 column (Σx2) and the sum of the y2 column (Σy2). The resulting formula is:

r= Σxy/ sqrt((Σx2)*(Σy2))

An alternative computational formula that avoids the step of computing deviation scores is:

r= (ΣXY - ΣXY/N) / sqrt(ΣX2 - (ΣX)2/N)*sqrt(ΣY2 - (ΣY)2/N)


Variance Sum Law II
Recall that when the variables X and Y are independent, the variance of the sum or difference between X and Y can be written as follows:

(σ2)X+/-Y = (σ2)X+(σ2)Y
==> var(X+/-Y) = var(X) + var(Y) 

which is read: "The variance of X plus or minus Y is equal to the variance of X plus the variance of Y."
When X and Y are correlated, the following formula should be used:

(σ2)X+/-Y = (σ2)X+(σ2)Y +/- 2ρσXσY

where ρ is the correlation between X and Y in the population. 

Pearson's correlation measures the strength of the linear relationship between two variables. 
The relationship here is not linear. As age increases, hours slept decreases rapidly at first but then levels off.

Part5. Probability
Inferential statistics is built on the foundation of probability theory, and has been remarkably successful in guiding opinion about the conclusions to be drawn from data. 
One conception of probability is drawn from the idea of symmetrical outcomes.
Probabilities can also be thought of in terms of relative frequencies. 
For some purposes, probability is best thought of as subjective. 

Probability of a Single Event
If you roll a six-sided die, there are six possible outcomes, and each of these outcomes is equally likely.
The two outcomes about which we are concerned (a one or a six coming up) are called favorable outcomes. 
Given that all outcomes are equally likely, we can compute the probability of a one or a six using the formula:

probability  = Number of favorable outcomes / Number of possible equally-likely outcomes

Probability of Two (or more) Independent Events
Events A and B are independent events if the probability of Event B occurring is the same whether or not Event A occurs.

Probability of A and B
When two events are independent, the probability of both occurring is the product of the probabilities of the individual events. 
More formally, if events A and B are independent, then the probability of both A and B occurring is:
P(A and B) = P(A) x P(B)

Probability of A or B
If Events A and B are independent, the probability that either Event A or Event B occurs is:
P(A or B) = P(A) + P(B) - P(A and B)

In this discussion, when we say "A or B occurs" we include three possibilities:
1. A occurs and B does not occur
2. B occurs and A does not occur
3. Both A and B occur

Conditional Probabilities
Often it is required to compute the probability of an event given that another event has occurred. 

Permutations and Combinations

Possible Orders
The formula for the number of orders is shown below.
Number of orders = n!

Multiplication Rule
Imagine a small restaurant whose menu has 3 soups, 6 entrées, and 4 desserts. 
How many possible meals are there? The answer is calculated by multiplying the numbers to get 3 x 6 x 4 = 72. 

Permutations
Suppose that there were four pieces of candy (red, yellow, green, and brown) and you were only going to pick up exactly two pieces. How many ways are there of picking up two pieces?

More formally, this question is asking for the number of permutations of four things taken two at a time. The general formula is:

nPr = n!/(n-r)!
nPr is the number of permutations of n things taken r at a time. In other words, it is the number of ways r things can be selected from a group of n things. 
It is important to note that order counts in permutations.
Therefore permutations refer to the number of ways of choosing rather than the number of possible outcomes. 
When order of choice is not considered, the formula for combinations is used.

Combinations

The formula for the number of combinations is shown below where nCr is the number of combinations for n things taken r at a time.
nCr = n!/(n-r)!r!

Binomial Distribution
we consider probability distributions for which there are just two possible outcomes with fixed probabilities summing to one. These distributions are called binomial distributions.

The Formula for Binomial Probabilities
The binomial distribution consists of the probabilities of each of the possible numbers of successes on N trials for independent events that each have a probability of π (the Greek letter pi) of occurring.

P(x) = N!/x!(N-x)! * (π)power(x) * (1-π)power(N-x)
P(x) = NCx * (π)power(x) * (1-π)power(N-x)

We toss a coin 12 times. What is the probability that we get from 0 to 3 heads? 
The answer is found by computing the probability of exactly 0 heads, exactly 1 head, exactly 2 heads, and exactly 3 heads. 
The probability of getting from 0 to 3 heads is then the sum of these probabilities.

Mean and Standard Deviation of Binomial Distributions
In general, the mean of a binomial distribution with parameters N (the number of trials) and π (the probability of success on each trial) is:
µ = Nπ

where μ is the mean of the binomial distribution. The variance of the binomial distribution is:
σ2 = Nπ(1-π)
σ2 is the variance of the binomial distribution.

Naturally, the standard deviation (σ) is the square root of the variance (σ2).
σ = sqrt(Nπ(1-π))

The binomial distribution is symmetric for p = 0.5.

 binomial distributions has the largest skew
The larger the difference between p and 0.5 and the smaller the N, the larger the skew

Poisson Distribution
The Poisson distribution can be used to calculate the probabilities of various numbers of "successes" based on the mean number of successes. 
In order to apply the Poisson distribution, the various events must be independent. 

Suppose you knew that the mean number of calls to a fire station on a weekday is 8. What is the probability that on a given weekday there would be 11 calls? This problem can be solved using the following formula based on the Poisson distribution:
poisson formula where

p = exp(-µ) * (µ)power(x) / x!

e is the base of natural logarithms (2.7183)
μ is the mean number of "successes"
x is the number of "successes" in question

The mean of the Poisson distribution is μ. The variance is also equal to μ. 

Multinomial Distribution
The binomial distribution allows one to compute the probability of obtaining a given number of binary outcomes. 
The multinomial distribution can be used to compute the probabilities in situations in which there are more than two possible outcomes. 

For example, suppose that two chess players had played numerous games and it was determined that the probability that Player A would win is 0.40, the probability that Player B would win is 0.35, and the probability that the game would end in a draw is 0.25. 
The multinomial distribution can be used to answer questions such as: "If these two chess players played 12 games, what is the probability that Player A would win 7 games, Player B would win 2 games, and the remaining 3 games would be drawn?" 
The following formula gives the probability of obtaining a specific set of outcomes when there are three possible outcomes for each event:

p= (n! / (n1!) * (n2!) *(n3!)) * p1**n1 * p2**n2 * p3**n3

p is the probability,
n is the total number of events
n1 is the number of times Outcome 1 occurs,
n2 is the number of times Outcome 2 occurs,
n3 is the number of times Outcome 3 occurs,
p1 is the probability of Outcome 1
p2 is the probability of Outcome 2, and
p3 is the probability of Outcome 3.

Hypergeometric Distribution
The hypergeometric distribution is used to calculate probabilities when sampling without replacement. 


For example, suppose you first randomly sample one card from a deck of 52. Then, without putting the card back in the deck you sample a second and then (again without replacing cards) a third. Given this sampling procedure, what is the probability that exactly two of the sampled cards will be aces (4 of the 52 cards in the deck are aces). You can calculate this probability using the following formula based on the hypergeometric distribution:
formula where

p = kCx * (N-K)C(n-x) / NCn

k is the number of "successes" in the population
x is the number of "successes" in the sample
N is the size of the population
n is the number sampled
p is the probability of obtaining exactly x successes
kCx is the number of combinations of k things taken x at a time

The mean and standard deviation of the hypergeometric distribution are:
µ = (n)(k)/N

sd = sqrt((n)(k)/N * (N-n)(N-k)/(N)(N-1))


Base Rates
For one thing, more information about the accuracy of the test is needed because there are two kinds of errors the test can make: misses and false positives. 

Miss
Misses occur when a diagnostic test returns a negative result, but the true state of the subject is positive. 
For example, if a person has strep throat and the diagnostic test fails to indicate it, then a miss has occurred. 
The concept is similar to a Type II error in significance testing.

False Positive
A false positive occurs when a diagnostic procedure returns a positive result while the true state of the subject is negative. 
For example, if a test for strep says the patient has strep when in fact he or she does not, then the error in diagnosis would be called a false positive. 
In some contexts, a false positive is called a false alarm. The concept is similar to a Type I error in significance testing.

The miss and false positive rates are not necessarily the same. 
For example, suppose that the test accurately indicates the disease in 99% of the people who have it and accurately indicates no disease in 91% of the people who do not have it. 
In other words, the test has a miss rate of 0.01 and a false positive rate of 0.09. This might lead you to revise your judgment and conclude that your chance of having the disease is 0.91.
This would not be correct since the probability depends on the proportion of people having the disease. This proportion is called the base rate.

Base Rate
The true proportion of a population having some condition, attribute or disease. 
For example, the proportion of people with schizophrenia is about 0.01. 
It is very important to consider the base rate when classifying people. 
As the saying goes, "if you hear hoofs, think horse not zebra" since you are more likely to encounter a horse than a zebra (at least in most places.)

Assume that Disease X is a rare disease, and only 2% of people in your situation have it.
How does that affect the probability that you have it? Or, more generally, what is the probability that someone who tests positive actually has the disease? 
Let's consider what would happen if one million people were tested. Out of these one million people, 2% or 20,000 people would have the disease. 
Of these 20,000 with the disease, the test would accurately detect it in 99% of them. This means that 19,800 cases would be accurately identified. 
Now let's consider the 98% of the one million people (980,000) who do not have the disease. Since the false positive rate is 0.09, 9% of these 980,000 people will test positive for the disease. 
This is a total of 88,200 people incorrectly diagnosed.
To sum up, 19,800 people who tested positive would actually have the disease and 88,200 people who tested positive would not have the disease. 
This means that of all those who tested positive, only   
19,800/(19,800 + 88,200) = 0.1833
of them would actually have the disease. So the probability that you have the disease is not 0.95, or 0.91, but only 0.1833.

			True Condition
No Disease  					Disease
980,000 						20,000
Test Result    			Test Result  
Positive  Negative       Positive  Negative  
88,200    891,800        19,800   200

sensitivity = TP/TP+FN = 19800/(891800 + 200)
specificity = TN/N = 891800/980000
precision = TP/TP+FP = 19800/ (19800 + 88200)
negative predictive = TN/TN+FN =  891800/( 891800+200)
accuracy = TP+TN/P+N = (891800 + 19800) / (980000+20000)
F1 = 2TP / (2TP + FP + FN)

Bayes' Theorem
This same result can be obtained using Bayes' theorem. 
Bayes' theorem considers both the prior probability of an event and the diagnostic value of a test to determine the posterior probability of the event.

Prior Probability
The prior probability of an event is the probability of the event computed before the collection of new data. 
One begins with a prior probability of an event and revises it in the light of new data. 

Posterior Probability
The posterior probability of an event is the probability of the event computed following the collection of new data.

For the current example, the event is that you have Disease X. Let's call this Event D. 
Since only 2% of people in your situation have Disease X, the prior probability of Event D is 0.02. Or, more formally, P(D) = 0.02. 
If P(D') represents the probability that Event D is false, then P(D') = 1 - P(D) = 0.98.

To define the diagnostic value of the test, we need to define another event: that you test positive for Disease X. Let's call this Event T. 
The diagnostic value of the test depends on the probability you will test positive given that you actually have the disease, written as P(T|D), 
and the probability you test positive given that you do not have the disease, written as P(T|D'). 
Bayes' theorem shown below allows you to calculate P(D|T), the probability that you have the disease given that you test positive for it.

P(D|T) = P(T|D)P(D)/(P(T|D)P(D) + P(T|D')P(D'))
P(D|T)P(T) = P(T|D)P(D)
