Online Statistics Education: An Interactive Multimedia Course of Study
http://onlinestatbook.com/2/index.html


Part1. Introduction

Inferential Statistics
In statistics, we often rely on a sample --- that is, a small subset of a larger set of data --- to draw inferences about the larger set. The larger set is known as the population.

Simple Random Sampling
Such sampling requires every member of the population to have an equal chance of being selected into the sample. 
In addition, the selection of one member must be independent of the selection of every other member. 
That is, picking one member from the population must not increase or decrease the probability of picking any other member (relative to the others). 

Random sampling chooses a sample by pure chance

More Complex Sampling
1. Random Assignment : used for experiment studies: experimental group vs control group, 
Random assignment is critical for the validity of an experiment. 
In experimental research of this kind, failure to assign subjects randomly to groups is generally more serious than having a non-random sample.
Failure to randomize (the former error) invalidates the experimental findings. A non-random sample (the latter error) simply restricts the generalizability of the results.

2. Stratified Sampling: 
In stratified sampling, you first identify members of your sample who belong to each group. 
Then you randomly sample from each of those subgroups in such a way that the sizes of the subgroups in the sample are proportional to their sizes in the population. 

Stratified sampling is more likely to be representative of the population than random sampling.
The only way to eliminate uncertainty is to obtain data from the whole population. You can reduce uncertainty with a large sample.
Using a random sample:
is to accept some uncertainty about the conclusions. 

enables you to calculate statistics. 

is to risk drawing the wrong conclusions about the population. 
 
A biased sample is one that 

will likely have groups from the population over-represented or under-represented due to systematic sampling factors. 
Bias is defined by the procedure for drawing the sample, not by the result.

Levels of Measurement
nominal, ordinal, interval, and ratio scales
ordinal scales fail to capture important informations
In particular, the difference between two levels of an ordinal scale cannot be assumed to be the same as the difference between two other levels.
Statisticians express this point by saying that the differences between adjacent scale values do not necessarily represent equal intervals on the underlying scale giving rise to the measurements.

Interval scales are numerical scales in which intervals have the same interpretation throughout. 
Interval scales are not perfect, however. In particular, they do not have a true zero point even if one of the scaled values happens to carry the name "zero." 
For example, there is no sense in which the ratio of 40 to 20 degrees Fahrenheit is the same as the ratio of 100 to 50 degrees; no interesting physical property is preserved across the two ratios. 
The ratio scale of measurement is the most informative scale. It is an interval scale with the additional property that its zero position indicates the absence of the quantity being measured. 
You can think of a ratio scale as the three earlier scales rolled up in one. Like a nominal scale, it provides a name or category for each object (the numbers serve as labels). 
Like an ordinal scale, the objects are ordered (in terms of the ordering of the numbers). Like an interval scale, the same difference at two places on the scale has the same meaning. 
And in addition, the same ratio at two places on the scale also carries the same meaning.

Rating scales are used frequently in psychological research.
Typically these ratings are made on a 5-point or a 7-point scale. These scales are ordinal scales since there is no assurance that a given difference represents the same thing across the range of the scale. 

Does it make sense to compute the mean of numbers measured on an ordinal scale? This is a difficult question, one that statisticians have debated for decades. 
The prevailing (but by no means unanimous) opinion of statisticians is that for almost all practical situations, the mean of an ordinally-measured variable is a meaningful statistic. 
However, as you will see in the simulation, there are extreme situations in which computing the mean of an ordinally-measured variable can be very misleading.

Ordinal scales preserve the order of the values, but not the differences between values.

http://onlinestatbook.com/2/introduction/measurement_demo.html
A difference between the means of two groups on an ordinal rating scale : 
For all but the most extraordinary situations, differences between means on interval scales are meaningful. 
, in extreme circumstances, it is possible for the difference on an ordinal scale to be in the oppostite direction from the difference on an interval scale.


Distributions
frequency distribution VS probability distribution
Distributions of Discrete Variables

Table 1. Frequencies in the Bag of M&M's
Color	Frequency
Brown	17
Red		18
Yellow	7
Green	7
Blue		2
Orange	4

This table is called a frequency table and it describes the distribution of M&M color frequencies. 
We call Figure 2 a probability distribution because if you choose an M&M at random, the probability of getting, say, a brown M&M is equal to the proportion of M&M's that are brown (0.30).

Chance factors involving the machines used by the manufacturer introduce random variation into the different bags produced. Some bags will have a distribution of colors that is close to Figure 2; 
others will be further away.

Continuous Variables
histogram can be used to show the grouped frequency distribution

Probability Densities (continuous distribution)
normal distribution: The Y-axis in the normal distribution represents the "density of probability." Intuitively, it shows the chance of obtaining values near corresponding points on the X-axis.

the curve that describes a continuous distribution (like the normal distribution). 
First, the area under the curve equals 1. 
Second, the probability of any exact value of X is 0. 
Finally, the area under the curve and bounded between two given points on the X-axis is the probability that a number chosen at random will fall between the two points.

A distribution with two peaks is called a bimodal distribution.
Distributions also differ from each other in terms of how large or "fat" their tails are. 
The upper distribution has relatively more scores in its tails; its shape is called leptokurtic. The lower distribution has relatively fewer scores in its tails; its shape is called platykurtic.


Part2. Graphing Distributions
Graphing Qualitative Variables

Pie charts are effective for displaying the relative frequencies of a small number of categories. 
Pie charts can also be confusing when they are used to compare the outcomes of two different surveys or experiments. 
In an influential book on the use of graphs, Edward Tufte asserted, "The only worse design than a pie chart is several of them."
we note that it is a serious mistake to use a line graph when the X-axis contains merely qualitative variables.

Pie charts and bar charts can both be effective methods of portraying qualitative data. 
Bar charts are better when there are more than just a few categories and for comparing two or more distributions

Quantitative Variables

Stem and Leaf Displays
Whether your data can be suitably represented by a stem and leaf graph depends on whether they can be rounded without loss of important information. 

Histograms

A histogram is a graphical method for displaying the shape of a distribution. It is particularly useful when there are a large number of observations. 

Table 1. Grouped Frequency Distribution of Psychology Test Scores
Interval's Lower Limit	Interval's Upper Limit	Class Frequency
39.5	49.5	3
49.5	59.5	10
59.5	69.5	53
69.5	79.5	107
79.5	89.5	147
89.5	99.5	130
99.5	109.5	78
109.5	119.5	59
119.5	129.5	36
129.5	139.5	11s
139.5	149.5	6
149.5	159.5	1
159.5	169.5	1

the range of scores was broken into intervals, called class intervals,
Class intervals of width 10 provide enough detail about the distribution to be revealing without making the graph too "choppy." 
More information on choosing the widths of class intervals is presented later in this section. 
Placing the limits of the class intervals midway between two numbers (e.g., 49.5) ensures that every score will fall in an interval rather than on the boundary between intervals.
The histogram makes it plain that most of the scores are in the middle of the distribution, with fewer scores in the extremes. 
Using whole numbers as boundaries avoids a cluttered appearance, and is the practice of many computer programs that create histograms. 
Note also that some computer programs label the middle of each interval rather than the end points.
Histograms can be based on relative frequencies instead of actual frequencies. Histograms based on relative frequencies show the proportion of scores in each interval rather than the number of scores. 

widths of the class intervals = bin widths

Your choice of bin width determines the number of class intervals. This decision, along with the choice of starting point for the first interval, affects the shape of the histogram.
Sturges' rule is to set the number of intervals as close as possible to 1 + Log2(N), where Log2(N) is the base 2 log of the number of observations. 
The formula can also be written as 1 + 3.3 Log10(N)
We prefer the Rice rule, which is to set the number of intervals to twice the cube root of the number of observations ******

The best advice is to experiment with different choices of width, and to choose a histogram according to how well it communicates the shape of the distribution.

Frequency Polygons
Frequency polygons are useful for comparing distributions. This is achieved by overlaying the frequency polygons drawn for different data sets. 
Frequency polygons are better at comparing distributions because two frequency polygons can be displayed in the same graph without obscuring each other. 
Both histograms and frequency polygons show the shape of the distribution. Neither necessarily reveals the exact values in a distribution.
Frequency polygons are a graphical device for understanding the shapes of distributions. They serve the same purpose as histograms, but are especially helpful for comparing sets of data.

Boxplot
Since half the scores in a distribution are between the hinges (recall that the hinges are the 25th and 75th percentiles), 
we see that half the women's times are between 17 and 20 seconds, whereas half the men's times are between 19 and 25.5. 
We also see that women generally named the colors faster than the men did, although one woman was slower than almost all of the men.

a distribution with a positive skew would have a longer whisker in the positive direction than in the negative direction. 
A larger mean than median would also indicate a positive skew. Box plots are good at portraying extreme values and are especially good at showing differences between distributions. 
However, many of the details of a distribution are not revealed in a box plot, and to examine these details one should create a histogram and/or a stem and leaf display.


Median and Mean

51) the point on which a distribution would balance, (2) the value whose average absolute deviation from all the other values is minimized, and (3) the value whose average squared difference from all the other values is minimized. 
From the simulation in this chapter, you discovered (we hope) that the mean is the point on which a distribution would balance, the median is the value that minimizes the sum of absolute deviations, and the mean is the value that minimizes the sum of the squared deviations.


Additional Measures of Central Tendency
Trimean
The trimean is a weighted average of the 25th percentile, the 50th percentile, and the 75th percentile. Letting P25 be the 25th percentile, P50 be the 50th and P75 be the 75th percentile, the formula for the trimean is:
Trimean = (P25 + 2P50 + P75)/4

Geometric Mean
The geometric mean is computed by multiplying all the numbers together and then taking the nth root of the product.
The geometric mean is an appropriate measure to use for averaging rates. For example, consider a stock portfolio that began with a value of $1,000 and had annual returns of 13%, 22%, 12%, -5%, and -13%. Table 4 shows the value after each of the five years.
Table 4. Portfolio Returns
Year	Return	Value
1	13%	1,130
2	22%	1,379
3	12%	1,544
4	-5%	1,467
5	-13%	1,276
The question is how to compute average annual rate of return. The answer is to compute the geometric mean of the returns. Instead of using the percents, each return is represented as a multiplier indicating how much higher the value is after the year. 
This multiplier is 1.13 for a 13% return and 0.95 for a 5% loss. The multipliers for this example are 1.13, 1.22, 1.12, 0.95, and 0.87. 
The geometric mean of these multipliers is 1.05. Therefore, the average annual rate of return is 5%. Table 5 shows how a portfolio gaining 5% a year would end up with the same value ($1,276) as shown in Table 4.

Trimmed Mean
To compute a trimmed mean, you remove some of the higher and lower scores and compute the mean of the remaining scores. A mean trimmed 10% is a mean computed with 10% of the scores trimmed off: 5% from the bottom and 5% from the top. 
A mean trimmed 50% is computed by trimming the upper 25% of the scores and the lower 25% of the scores and computing the mean of the remaining scores. The trimmed mean is similar to the median which, in essence, trims the upper 49+% and the lower 49+% of the scores. 
Therefore the trimmed mean is a hybrid of the mean and the median. To compute the mean trimmed 20% for the touchdown pass data shown in Table 1, you remove the lower 10% of the scores (6, 9, and 12) as well as the upper 10% of the scores (33, 33, and 37) and compute the mean of the remaining 25 scores. This mean is 20.16.


Fortunately, there is no need to summarize a distribution with a single number. When the various measures differ, our opinion is that you should report the mean, median, and either the trimean or the mean trimmed 50%. 
Sometimes it is worth reporting the mode as well. In the media, the median is usually reported to summarize the center of skewed distributions. 

Measures of Variability

What is Variability?
Variability refers to how "spread out" a group of scores is.
There are four frequently used measures of variability: the range, interquartile range, variance, and standard deviation.
 
Range
The range is the simplest measure of variability to calculate, and one you have probably encountered many times in your life. 
The range is simply the highest score minus the lowest score.
Let’s take a few examples. What is the range of the following group of numbers: 10, 2, 5, 6, 7, 3, 4? 
Well, the highest number is 10, and the lowest number is 2, so 10 - 2 = 8. The range is 8.

Interquartile Range
The interquartile range (IQR) is the range of the middle 50% of the scores in a distribution. It is computed as follows:
IQR = 75th percentile - 25th percentile
Recall that in the discussion of box plots, the 75th percentile was called the upper hinge and the 25th percentile was called the lower hinge.
Using this terminology, the interquartile range is referred to as the H-spread.
 
A related measure of variability is called the semi-interquartile range. The semi-interquartile range is defined simply as the interquartile range divided by 2. 
If a distribution is symmetric, the median plus or minus the semi-interquartile range contains half the scores in the distribution. 

Variance
Variability can also be defined in terms of how close the scores in the distribution are to the middle of the distribution. 
Using the mean as the measure of the middle of the distribution, the variance is defined as the average squared difference of the scores from the mean.
One thing that is important to notice is that the mean deviation from the mean is 0. This will always be the case. 

where σ2 is the variance, μ is the mean, and N is the number of numbers. 
If the variance in a sample is used to estimate the variance in a population, then the previous formula underestimates the variance and the following formula should be used:

N-1 to calculate the variance
where s2 is the estimate of the variance and M is the sample mean. 
Note that M is the mean of a sample taken from a population with a mean of μ. Since, in practice, the variance is usually computed in a sample, this formula is most often used. 

Standard Deviation
The standard deviation is simply the square root of the variance.
he standard deviation is an especially useful measure of variability when the distribution is normal or approximately normal (see Chapter on Normal Distributions) because the proportion of the distribution within a given number of standard deviations from the mean can be calculated. 
For example, 68% of the distribution is within one standard deviation of the mean and approximately 95% of the distribution is within two standard deviations of the mean.
Therefore, if you had a normal distribution with a mean of 50 and a standard deviation of 10, then 68% of the distribution would be between 50 - 10 = 40 and 50 +10 =60. Similarly, about 95% of the distribution would be between 50 - 2 x 10 = 30 and 50 + 2 x 10 = 70. 
The symbol for the population standard deviation is σ; the symbol for an estimate computed in a sample is s. 

Assume you repeatedly sampled 4 numbers from this population with a variance of 2 and, for each sample, estimated the variance using the average squared difference from the sample mean. 
What would the mean of these variance estimates be?
It would approach 1.5 as the number of samples increases to a very large number. 
This is equal to (N-1)/N = .75 times the population variance of 2.

Shapes of Distributions
The relationship between skew and the relative size of the mean and median led the statistician Pearson to propose the following simple and convenient numerical index of skew:

3(mean-median)/σ

Just as there are several measures of central tendency, there is more than one measure of skew. 
Although Pearson's measure is a good one, the following measure is more commonly used. It is sometimes referred to as the third moment about the mean.

sigma[(X-µ)3/σ3]

Kurtosis
The following measure of kurtosis is similar to the definition of skew. 
The value "3" is subtracted to define "no kurtosis" as the kurtosis of a normal distribution. Otherwise, a normal distribution would have a kurtosis of 3.

sigma[(X-µ)4/σ4]-3

Effects of Linear Transformations

To sum up, if a variable X has a mean of μ, a standard deviation of σ, and a variance of σ2, then a new variable Y created using the linear transformation
Y = bX + A

will have a mean of bμ+A, a standard deviation of bσ, and a variance of b2σ2.


